{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3Q-d5IjPFWY"
      },
      "source": [
        "# Constraint-Guided Decoding for Small Language Models\n",
        "\n",
        "**Author:** Nolan W. Platt  \n",
        "**Project:** Lightweight Neurosymbolic Approach for SLMs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xMpmv-iPFWZ"
      },
      "source": [
        "## Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TT9VHOkPFWZ"
      },
      "outputs": [],
      "source": [
        "# install required packages\n",
        "!pip install -q transformers accelerate bitsandbytes z3-solver torch sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R1F8nFWPFWZ",
        "outputId": "fd9e3dfe-1b01-4a2f-afbd-134cf10518bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from z3 import *\n",
        "import re\n",
        "from typing import List, Dict, Tuple, Set, Optional\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZHMN_ynPFWa"
      },
      "source": [
        "## Z3-Based Constraint Checker\n",
        "\n",
        "Implementation of the constraint engine with:\n",
        "1. Arithmetic constraint checking\n",
        "2. Logical consistency verification\n",
        "3. Variable tracking and extraction\n",
        "4. Constraint caching for efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdyQBplVPFWa"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Constraint:\n",
        "    \"\"\"Represents a logical constraint\"\"\"\n",
        "    constraint_type: str  # 'arithmetic', 'logical', 'syntax'\n",
        "    expression: str\n",
        "    variables: Set[str]\n",
        "    z3_formula: Optional[Any] = None\n",
        "\n",
        "class ConstraintChecker:\n",
        "    \"\"\"Z3-based constraint verification engine\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.constraint_cache = {}\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"Reset solver state\"\"\"\n",
        "        self.constraint_cache = {}\n",
        "\n",
        "    def extract_arithmetic_statements(self, text: str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Extract arithmetic statements from text\n",
        "        Examples: \"x = 5\", \"y = 10\", \"x + y = 15\"\n",
        "        \"\"\"\n",
        "        statements = []\n",
        "\n",
        "        # Pattern: variable = number (but not part of an expression)\n",
        "        # using word boundaries and negative lookahead/lookbehind to avoid matching inside expressions\n",
        "        simple_pattern = r'(?<![+\\-*/])\\s*([a-zA-Z])\\s*=\\s*(\\d+(?:\\.\\d+)?)\\s*(?![+\\-*/])'\n",
        "\n",
        "        for match in re.finditer(simple_pattern, text):\n",
        "            var_name = match.group(1)\n",
        "            value = match.group(2)\n",
        "\n",
        "            # Skip if this looks like it's part of a larger expression\n",
        "            start = match.start()\n",
        "            if start > 0 and text[start-1] in '+-*/':\n",
        "                continue\n",
        "\n",
        "            statements.append({\n",
        "                'variable': var_name,\n",
        "                'value': float(value),\n",
        "                'full_text': match.group(0)\n",
        "            })\n",
        "\n",
        "        return statements\n",
        "\n",
        "    def check_arithmetic_consistency(self, text: str) -> Tuple[bool, List[str]]:\n",
        "        \"\"\"\n",
        "        Check if all arithmetic statements in text are consistent\n",
        "        Returns: (is_consistent, list_of_violations)\n",
        "        \"\"\"\n",
        "        cache_key = hash(text)\n",
        "        if cache_key in self.constraint_cache:\n",
        "            return self.constraint_cache[cache_key]\n",
        "\n",
        "        statements = self.extract_arithmetic_statements(text)\n",
        "        if not statements:\n",
        "            result = (True, [])\n",
        "            self.constraint_cache[cache_key] = result\n",
        "            return result\n",
        "\n",
        "        # build variable assignments\n",
        "        var_assignments = {}\n",
        "        violations = []\n",
        "\n",
        "        for stmt in statements:\n",
        "            var_name = stmt['variable']\n",
        "            value = stmt['value']\n",
        "\n",
        "            if var_name in var_assignments:\n",
        "                # check if this contradicts previous assignment\n",
        "                if abs(var_assignments[var_name] - value) > 0.001:\n",
        "                    violations.append(\n",
        "                        f\"Contradiction: {var_name} = {var_assignments[var_name]} \"\n",
        "                        f\"but later {var_name} = {value}\"\n",
        "                    )\n",
        "            else:\n",
        "                var_assignments[var_name] = value\n",
        "\n",
        "        #  check expressions like \"x + y = 15\"\n",
        "        expr_pattern = r'([a-zA-Z_]\\w*)\\s*\\+\\s*([a-zA-Z_]\\w*)\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
        "        for match in re.finditer(expr_pattern, text):\n",
        "            var1 = match.group(1)\n",
        "            var2 = match.group(2)\n",
        "            expected = float(match.group(3))\n",
        "\n",
        "            if var1 in var_assignments and var2 in var_assignments:\n",
        "                actual = var_assignments[var1] + var_assignments[var2]\n",
        "                if abs(actual - expected) > 0.001:\n",
        "                    violations.append(\n",
        "                        f\"Arithmetic error: {var1} + {var2} should be {actual} \"\n",
        "                        f\"but text claims {expected}\"\n",
        "                    )\n",
        "\n",
        "        # check multiplication like \"2x = 10\"\n",
        "        mult_pattern = r'(\\d+)\\s*\\*?\\s*([a-zA-Z_]\\w*)\\s*=\\s*(\\d+(?:\\.\\d+)?)'\n",
        "        for match in re.finditer(mult_pattern, text):\n",
        "            multiplier = float(match.group(1))\n",
        "            var_name = match.group(2)\n",
        "            expected = float(match.group(3))\n",
        "\n",
        "            if var_name in var_assignments:\n",
        "                actual = multiplier * var_assignments[var_name]\n",
        "                if abs(actual - expected) > 0.001:\n",
        "                    violations.append(\n",
        "                        f\"Arithmetic error: {multiplier}*{var_name} should be {actual} \"\n",
        "                        f\"but text claims {expected}\"\n",
        "                    )\n",
        "\n",
        "        is_consistent = len(violations) == 0\n",
        "        result_tuple = (is_consistent, violations)\n",
        "        self.constraint_cache[cache_key] = result_tuple\n",
        "        return result_tuple\n",
        "\n",
        "    def check_logical_consistency(self, text: str) -> Tuple[bool, List[str]]:\n",
        "        \"\"\"\n",
        "        Check for logical contradictions\n",
        "        \"\"\"\n",
        "        violations = []\n",
        "\n",
        "        # true/false statements regex patterns\n",
        "        positive_pattern = r'([A-Z]\\w*)\\s+is\\s+(true|correct)'\n",
        "        negative_pattern = r'([A-Z]\\w*)\\s+is\\s+(false|incorrect|not\\s+true)'\n",
        "\n",
        "        positive_matches = [(m.group(1).lower(), m.group(0)) for m in re.finditer(positive_pattern, text, re.IGNORECASE)]\n",
        "        negative_matches = [(m.group(1).lower(), m.group(0)) for m in re.finditer(negative_pattern, text, re.IGNORECASE)]\n",
        "\n",
        "        positive_props = {match[0] for match in positive_matches}\n",
        "        negative_props = {match[0] for match in negative_matches}\n",
        "\n",
        "        contradictions = positive_props & negative_props\n",
        "        if contradictions:\n",
        "            violations.append(f\"Logical contradiction for propositions: {contradictions}\")\n",
        "            return (False, violations)\n",
        "\n",
        "        return (True, [])\n",
        "\n",
        "    def check_constraints(self, text: str, constraint_types: List[str] = None) -> Tuple[bool, List[str]]:\n",
        "        \"\"\"\n",
        "        Main constraint checking interface\n",
        "        \"\"\"\n",
        "        if constraint_types is None:\n",
        "            constraint_types = ['arithmetic', 'logical']\n",
        "\n",
        "        all_violations = []\n",
        "        is_consistent = True\n",
        "\n",
        "        if 'arithmetic' in constraint_types:\n",
        "            arith_consistent, arith_violations = self.check_arithmetic_consistency(text)\n",
        "            if not arith_consistent:\n",
        "                is_consistent = False\n",
        "                all_violations.extend(arith_violations)\n",
        "\n",
        "        if 'logical' in constraint_types:\n",
        "            logic_consistent, logic_violations = self.check_logical_consistency(text)\n",
        "            if not logic_consistent:\n",
        "                is_consistent = False\n",
        "                all_violations.extend(logic_violations)\n",
        "\n",
        "        return (is_consistent, all_violations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGsM3j6xPFWa"
      },
      "source": [
        "### Test the Constraint Checker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czas8v00PFWb"
      },
      "source": [
        "## Phi-2 Integration with Constraint-Guided Beam Search\n",
        "\n",
        "Implementation of Algorithm 1 from the paper with:\n",
        "1. 4-bit quantized Phi-2 model loading\n",
        "2. Modified beam search with constraint verification\n",
        "3. Incremental constraint checking\n",
        "4. Early pruning for efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "12abba522a854f0d872136f5dd7a225c",
            "51ecc6765e6e44c0b68d40c7781519f3",
            "4bbffcbe357d44d6a26fc3af8623ade3",
            "6b8e848ef389400eab53e5df05f3dca0",
            "190612aefba541cdbbed86209be9a5b3",
            "72fe00d8ce4442bbb62055678e0bcd48",
            "45084dd48697466f83f1db04859a5477",
            "ab175e5bfa864d478a1f1a9c9e396234",
            "8bb72a7c22814282815d43f51313f1ad",
            "1963f235c95e43e9b256ce253cf0fe0f",
            "10202f216e6a472a8a481a653e973bc7"
          ]
        },
        "id": "RyYK_Vv9PFWb",
        "outputId": "badd3341-b4e0-4245-f68c-318c95a07680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Phi-2 model with 4-bit quantization...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12abba522a854f0d872136f5dd7a225c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model loaded successfully!\n",
            "  Model size: 1.78 GB\n"
          ]
        }
      ],
      "source": [
        "# load Phi-2 with 4-bit quantization\n",
        "print(\"Loading Phi-2 model with 4-bit quantization...\")\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model_name = \"microsoft/phi-2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "print(f\"✓ Model loaded successfully!\")\n",
        "print(f\"  Model size: {model.get_memory_footprint() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOQ3Q6-fPFWb"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class BeamState:\n",
        "    \"\"\"State for a single beam during search\"\"\"\n",
        "    sequence: List[int]  # token IDs\n",
        "    text: str  # decoded text\n",
        "    score: float  # log probability\n",
        "    violations: Set[str]  # accumulated constraint violations\n",
        "    is_finished: bool = False\n",
        "\n",
        "class ConstraintGuidedDecoder:\n",
        "    \"\"\"Modified beam search with constraint enforcement (Algorithm 1)\"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, constraint_checker, beam_width=4, lambda_weight=1.0):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.checker = constraint_checker\n",
        "        self.beam_width = beam_width\n",
        "        self.lambda_weight = lambda_weight\n",
        "\n",
        "    def get_top_k_tokens(self, input_ids: torch.Tensor, k: int) -> List[Tuple[int, float]]:\n",
        "        \"\"\"\n",
        "        Get top-k next tokens with their log probabilities\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]  # get logits for last position\n",
        "\n",
        "            #  softmax to get probabilities\n",
        "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "            log_probs = torch.log(probs)\n",
        "\n",
        "            #  top-k\n",
        "            top_log_probs, top_indices = torch.topk(log_probs, k, dim=-1)\n",
        "\n",
        "            # -> list of (token_id, log_prob) tuples\n",
        "            results = []\n",
        "            for idx, log_prob in zip(top_indices[0].cpu().tolist(), top_log_probs[0].cpu().tolist()):\n",
        "                results.append((idx, log_prob))\n",
        "\n",
        "            return results\n",
        "\n",
        "    def calculate_constraint_penalty(self, text: str, prev_violations: Set[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calculate phi(y_t) from Equation 1\n",
        "        Returns: 0 if constraints satisfied, or -inf if new violations\n",
        "        \"\"\"\n",
        "        is_consistent, new_violations = self.checker.check_constraints(text)\n",
        "\n",
        "        # convert violations list to set\n",
        "        current_violations = set(new_violations)\n",
        "\n",
        "        # check if new violations (Eq. 2)\n",
        "        if not current_violations.issubset(prev_violations):\n",
        "            return float('-inf')  # new violations - prune beam\n",
        "\n",
        "        return 0.0  # No new violations\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_new_tokens: int = 100,\n",
        "        constraint_types: List[str] = None,\n",
        "        temperature: float = 1.0,\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Main constraint-guided generation (Algorithm 1)\n",
        "\n",
        "        Returns:\n",
        "            dict with 'text', 'score', 'violations', 'stats'\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # initialiaze\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        initial_state = BeamState(\n",
        "            sequence=input_ids[0].tolist(),\n",
        "            text=prompt,\n",
        "            score=0.0,\n",
        "            violations=set()\n",
        "        )\n",
        "\n",
        "        beams = [initial_state]\n",
        "\n",
        "        tokens_generated = 0\n",
        "        candidates_pruned = 0\n",
        "\n",
        "        # beam search loop\n",
        "        for step in range(max_new_tokens):\n",
        "            if all(beam.is_finished for beam in beams):\n",
        "                break\n",
        "\n",
        "            all_candidates = []\n",
        "\n",
        "            # expand each beam\n",
        "            for beam in beams:\n",
        "                if beam.is_finished:\n",
        "                    # keep finished beams, nothing else to do\n",
        "                    all_candidates.append(beam)\n",
        "                    continue\n",
        "\n",
        "                # get top 2k tokens for this beam\n",
        "                beam_input_ids = torch.tensor([beam.sequence]).to(self.model.device)\n",
        "                top_tokens = self.get_top_k_tokens(beam_input_ids, k=2 * self.beam_width)\n",
        "\n",
        "                # eval each token\n",
        "                for token_id, log_prob in top_tokens:\n",
        "                    # create new seq\n",
        "                    new_sequence = beam.sequence + [token_id]\n",
        "                    new_text = self.tokenizer.decode(new_sequence, skip_special_tokens=True)\n",
        "\n",
        "                    # check constraints incrementally\n",
        "                    constraint_penalty = self.calculate_constraint_penalty(\n",
        "                        new_text, beam.violations\n",
        "                    )\n",
        "\n",
        "                    # prune iff constraints violated (Eq. 1)\n",
        "                    if constraint_penalty == float('-inf'):\n",
        "                        candidates_pruned += 1\n",
        "                        continue\n",
        "\n",
        "                    # calc score w/ constraint penalty\n",
        "                    new_score = beam.score + log_prob + self.lambda_weight * constraint_penalty\n",
        "\n",
        "                    # check if sequence is finished\n",
        "                    is_finished = (token_id == self.tokenizer.eos_token_id)\n",
        "\n",
        "                    # gen new beam state\n",
        "                    new_beam = BeamState(\n",
        "                        sequence=new_sequence,\n",
        "                        text=new_text,\n",
        "                        score=new_score,\n",
        "                        violations=beam.violations.copy(),\n",
        "                        is_finished=is_finished\n",
        "                    )\n",
        "\n",
        "                    all_candidates.append(new_beam)\n",
        "\n",
        "            # get top-k candidates\n",
        "            all_candidates.sort(key=lambda x: x.score, reverse=True)\n",
        "            beams = all_candidates[:self.beam_width]\n",
        "\n",
        "            tokens_generated += 1\n",
        "\n",
        "        # return best complete sequence\n",
        "        best_beam = max(beams, key=lambda x: x.score)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        return {\n",
        "            'text': best_beam.text,\n",
        "            'score': best_beam.score,\n",
        "            'violations': list(best_beam.violations),\n",
        "            'stats': {\n",
        "                'tokens_generated': tokens_generated,\n",
        "                'candidates_pruned': candidates_pruned,\n",
        "                'time_seconds': end_time - start_time,\n",
        "                'final_beam_count': len(beams)\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og2BFywuPFWc"
      },
      "source": [
        "### Initialize the Constraint-Guided Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StZVvu1SPFWc",
        "outputId": "0283f540-6a6d-4e1d-fff7-52cb9be1bc07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Constraint-Guided Decoder initialized!\n"
          ]
        }
      ],
      "source": [
        "# create decoder instance\n",
        "constraint_checker = ConstraintChecker()\n",
        "decoder = ConstraintGuidedDecoder(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    constraint_checker=constraint_checker,\n",
        "    beam_width=6,  # can reduce  this width to 2 if OOM\n",
        "    lambda_weight=1.0\n",
        ")\n",
        "\n",
        "print(\"✓ Constraint-Guided Decoder initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhIpUkvqPFWc"
      },
      "source": [
        "## Testing and Validation\n",
        "\n",
        "Test the complete pipeline with arithmetic reasoning examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffrWitQWPFWc",
        "outputId": "3469bc72-9b28-4421-850d-b244f7ac7b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TESTS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "TEST 1: Basic Arithmetic Consistency\n",
            "================================================================================\n",
            "Setup: Give model x = 5, ask for 2x\n",
            "Expected: Should output 10 (2*5=10)\n",
            "Testing: Does constraint system prevent wrong answers?\n",
            "\n",
            "Running constrained generation...\n",
            "\n",
            "--- CONSTRAINED OUTPUT ---\n",
            "Full text: x = 5. Calculate 2x.\n",
            "Answer: 2x = (2 * 5) = 10.\n",
            "\n",
            "Exercise 2: Simplify the expression 3(\n",
            "\n",
            "Constraint violations: 0\n",
            "Candidates pruned: 10\n",
            "Generation time: 9.89s\n",
            "\n",
            "--- RESULT ---\n",
            "Contains '10': YES\n",
            "Status: PASS\n",
            "\n",
            "\n",
            "================================================================================\n",
            "TEST 2: Baseline vs Constrained Comparison\n",
            "================================================================================\n",
            "Setup: x = 7, y = 3, calculate x + y\n",
            "Expected: Both should output 10\n",
            "Testing: Does constraint add overhead? Does it help accuracy?\n",
            "\n",
            "Running BASELINE (standard beam search, no constraints)...\n",
            "\n",
            "--- BASELINE OUTPUT ---\n",
            "Full text: Given x = 7 and y = 3, what is x + y?\n",
            "Answer: x + y = 7 + 3 = 10\n",
            "\n",
            "Exercise 2: Solve the equation 2x - 5 =\n",
            "Constraint violations: 2\n",
            "  - Contradiction: y = 3.0 but later y = 7.0\n",
            "  - Arithmetic error: x + y should be 10.0 but text claims 7.0\n",
            "Contains '10': YES\n",
            "Generation time: 1.66s\n",
            "\n",
            "Running CONSTRAINED generation...\n",
            "\n",
            "--- CONSTRAINED OUTPUT ---\n",
            "Full text: Given x = 7 and y = 3, what is x + y?\n",
            "Answer: x + y = (7) + (3) = 10\n",
            "\n",
            "Exercise 2: Simplify the expression:\n",
            "Constraint violations: 0\n",
            "Contains '10': YES\n",
            "Candidates pruned: 56\n",
            "Generation time: 9.81s\n",
            "\n",
            "--- COMPARISON ---\n",
            "Baseline violations: 2\n",
            "Constrained violations: 0\n",
            "Time overhead: 8.15s (491.1% slower)\n",
            "IMPROVEMENT: Reduced 2 violation(s)\n",
            "\n",
            "\n",
            "================================================================================\n",
            "TEST 3: Preventing Contradictions\n",
            "================================================================================\n",
            "Setup: State a = 6, then ask model to calculate 2a\n",
            "Expected: Should NOT say things like '2a = 15' (would be wrong)\n",
            "Testing: Can we prevent the model from generating false arithmetic?\n",
            "\n",
            "Running constrained generation...\n",
            "\n",
            "--- CONSTRAINED OUTPUT ---\n",
            "Full text: We know that a = 6. Now calculate 2a.\n",
            "Solution: Since a = 6, we have 2a = (2)(6) = 12.\n",
            "\n",
            "\n",
            "Constraint violations: 0\n",
            "Candidates pruned: 10\n",
            "\n",
            "--- RESULT ---\n",
            "Expected answer (12): FOUND ✓\n",
            "No contradictions: YES ✓\n",
            "Status: PASS\n",
            "\n",
            "\n",
            "================================================================================\n",
            "TEST 4: Multi-Variable Arithmetic\n",
            "================================================================================\n",
            "Setup: m = 4, n = 5, calculate m * n\n",
            "Expected: Should output 20\n",
            "Testing: Can handle multiple variables correctly?\n",
            "\n",
            "--- CONSTRAINED OUTPUT ---\n",
            "Full text: Let m = 4 and n = 5. What is m * n?\n",
            "Answer: m * n = (4) * (5) = 20\n",
            "\n",
            "Exercise 2: Evaluate the following expression\n",
            "Violations: 0\n",
            "Contains '20': YES ✓\n",
            "Status: PASS\n",
            "\n",
            "\n",
            "================================================================================\n",
            "SUMMARY\n",
            "================================================================================\n",
            "PASS: Test 1: Basic arithmetic (x=5, 2x=?)\n",
            "PASS: Test 2: Reduced violations vs baseline\n",
            "PASS: Test 3: Contradiction prevention (a=6, 2a=?)\n",
            "PASS: Test 4: Multi-variable (m=4,n=5,m*n=?)\n",
            "\n",
            "Overall: 4/4 tests passed\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import time\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TESTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ========================================================================\n",
        "# TEST 1: Basic Arithmetic Consistency\n",
        "# ========================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEST 1: Basic Arithmetic Consistency\")\n",
        "print(\"=\"*80)\n",
        "print(\"Setup: Give model x = 5, ask for 2x\")\n",
        "print(\"Expected: Should output 10 (2*5=10)\")\n",
        "print(\"Testing: Does constraint system prevent wrong answers?\\n\")\n",
        "\n",
        "prompt1 = \"x = 5. Calculate 2x.\\nAnswer: 2x =\"\n",
        "\n",
        "# Run constrained generation\n",
        "print(\"Running constrained generation...\")\n",
        "start = time.time()\n",
        "constrained_result = decoder.generate(\n",
        "    prompt=prompt1,\n",
        "    max_new_tokens=20,\n",
        "    constraint_types=['arithmetic']\n",
        ")\n",
        "constrained_time = time.time() - start\n",
        "\n",
        "print(f\"\\n--- CONSTRAINED OUTPUT ---\")\n",
        "print(f\"Full text: {constrained_result['text']}\")\n",
        "print(f\"\\nConstraint violations: {len(constrained_result['violations'])}\")\n",
        "if constrained_result['violations']:\n",
        "    for v in constrained_result['violations']:\n",
        "        print(f\"  - {v}\")\n",
        "print(f\"Candidates pruned: {constrained_result['stats']['candidates_pruned']}\")\n",
        "print(f\"Generation time: {constrained_time:.2f}s\")\n",
        "\n",
        "# Check if answer is correct\n",
        "constrained_correct = \"10\" in constrained_result['text']\n",
        "\n",
        "print(f\"\\n--- RESULT ---\")\n",
        "print(f\"Contains '10': {'YES' if constrained_correct else 'NO'}\")\n",
        "print(f\"Status: {'PASS' if constrained_correct and len(constrained_result['violations'])==0 else 'FAIL'}\")\n",
        "\n",
        "# ========================================================================\n",
        "# TEST 2: Baseline Comparison (No Constraints)\n",
        "# ========================================================================\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"TEST 2: Baseline vs Constrained Comparison\")\n",
        "print(\"=\"*80)\n",
        "print(\"Setup: x = 7, y = 3, calculate x + y\")\n",
        "print(\"Expected: Both should output 10\")\n",
        "print(\"Testing: Does constraint add overhead? Does it help accuracy?\\n\")\n",
        "\n",
        "prompt2 = \"Given x = 7 and y = 3, what is x + y?\\nAnswer: x + y =\"\n",
        "\n",
        "# Baseline (no constraints)\n",
        "print(\"Running BASELINE (standard beam search, no constraints)...\")\n",
        "input_ids = tokenizer.encode(prompt2, return_tensors=\"pt\").to(model.device)\n",
        "start = time.time()\n",
        "baseline_output = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=20,\n",
        "    num_beams=4,\n",
        "    early_stopping=True,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    do_sample=False\n",
        ")\n",
        "baseline_time = time.time() - start\n",
        "baseline_text = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
        "\n",
        "# Check baseline for violations\n",
        "baseline_consistent, baseline_viols = constraint_checker.check_constraints(baseline_text, ['arithmetic'])\n",
        "\n",
        "print(f\"\\n--- BASELINE OUTPUT ---\")\n",
        "print(f\"Full text: {baseline_text}\")\n",
        "print(f\"Constraint violations: {len(baseline_viols)}\")\n",
        "if baseline_viols:\n",
        "    for v in baseline_viols:\n",
        "        print(f\"  - {v}\")\n",
        "print(f\"Contains '10': {'YES' if '10' in baseline_text else 'NO'}\")\n",
        "print(f\"Generation time: {baseline_time:.2f}s\")\n",
        "\n",
        "# Constrained\n",
        "print(\"\\nRunning CONSTRAINED generation...\")\n",
        "start = time.time()\n",
        "constrained_result2 = decoder.generate(\n",
        "    prompt=prompt2,\n",
        "    max_new_tokens=20,\n",
        "    constraint_types=['arithmetic']\n",
        ")\n",
        "constrained_time2 = time.time() - start\n",
        "\n",
        "print(f\"\\n--- CONSTRAINED OUTPUT ---\")\n",
        "print(f\"Full text: {constrained_result2['text']}\")\n",
        "print(f\"Constraint violations: {len(constrained_result2['violations'])}\")\n",
        "if constrained_result2['violations']:\n",
        "    for v in constrained_result2['violations']:\n",
        "        print(f\"  - {v}\")\n",
        "print(f\"Contains '10': {'YES' if '10' in constrained_result2['text'] else 'NO'}\")\n",
        "print(f\"Candidates pruned: {constrained_result2['stats']['candidates_pruned']}\")\n",
        "print(f\"Generation time: {constrained_time2:.2f}s\")\n",
        "\n",
        "# Comparison\n",
        "print(f\"\\n--- COMPARISON ---\")\n",
        "print(f\"Baseline violations: {len(baseline_viols)}\")\n",
        "print(f\"Constrained violations: {len(constrained_result2['violations'])}\")\n",
        "print(f\"Time overhead: {constrained_time2 - baseline_time:.2f}s ({((constrained_time2/baseline_time - 1)*100):.1f}% slower)\")\n",
        "\n",
        "improvement = len(baseline_viols) - len(constrained_result2['violations'])\n",
        "if improvement > 0:\n",
        "    print(f\"IMPROVEMENT: Reduced {improvement} violation(s)\")\n",
        "elif improvement < 0:\n",
        "    print(f\"WORSE: Added {abs(improvement)} violation(s)\")\n",
        "else:\n",
        "    print(f\"= SAME: No difference in violations\")\n",
        "\n",
        "# ========================================================================\n",
        "# TEST 3: Contradiction Prevention\n",
        "# ========================================================================\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"TEST 3: Preventing Contradictions\")\n",
        "print(\"=\"*80)\n",
        "print(\"Setup: State a = 6, then ask model to calculate 2a\")\n",
        "print(\"Expected: Should NOT say things like '2a = 15' (would be wrong)\")\n",
        "print(\"Testing: Can we prevent the model from generating false arithmetic?\\n\")\n",
        "\n",
        "prompt3 = \"We know that a = 6. Now calculate 2a.\\nSolution: Since a = 6, we have 2a =\"\n",
        "\n",
        "print(\"Running constrained generation...\")\n",
        "constrained_result3 = decoder.generate(\n",
        "    prompt=prompt3,\n",
        "    max_new_tokens=25,\n",
        "    constraint_types=['arithmetic']\n",
        ")\n",
        "\n",
        "print(f\"\\n--- CONSTRAINED OUTPUT ---\")\n",
        "print(f\"Full text: {constrained_result3['text']}\")\n",
        "print(f\"\\nConstraint violations: {len(constrained_result3['violations'])}\")\n",
        "if constrained_result3['violations']:\n",
        "    for v in constrained_result3['violations']:\n",
        "        print(f\"  - {v}\")\n",
        "print(f\"Candidates pruned: {constrained_result3['stats']['candidates_pruned']}\")\n",
        "\n",
        "# Check correctness (2*6 = 12)\n",
        "correct_answer = \"12\" in constrained_result3['text']\n",
        "\n",
        "print(f\"\\n--- RESULT ---\")\n",
        "print(f\"Expected answer (12): {'FOUND ✓' if correct_answer else 'NOT FOUND ✗'}\")\n",
        "print(f\"No contradictions: {'YES ✓' if len(constrained_result3['violations'])==0 else 'NO ✗'}\")\n",
        "print(f\"Status: {'PASS' if correct_answer and len(constrained_result3['violations'])==0 else 'FAIL'}\")\n",
        "\n",
        "# ========================================================================\n",
        "# TEST 4: Multi-Variable Consistency\n",
        "# ========================================================================\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"TEST 4: Multi-Variable Arithmetic\")\n",
        "print(\"=\"*80)\n",
        "print(\"Setup: m = 4, n = 5, calculate m * n\")\n",
        "print(\"Expected: Should output 20\")\n",
        "print(\"Testing: Can handle multiple variables correctly?\\n\")\n",
        "\n",
        "prompt4 = \"Let m = 4 and n = 5. What is m * n?\\nAnswer: m * n =\"\n",
        "\n",
        "constrained_result4 = decoder.generate(\n",
        "    prompt=prompt4,\n",
        "    max_new_tokens=20,\n",
        "    constraint_types=['arithmetic']\n",
        ")\n",
        "\n",
        "print(f\"--- CONSTRAINED OUTPUT ---\")\n",
        "print(f\"Full text: {constrained_result4['text']}\")\n",
        "print(f\"Violations: {len(constrained_result4['violations'])}\")\n",
        "print(f\"Contains '20': {'YES ✓' if '20' in constrained_result4['text'] else 'NO ✗'}\")\n",
        "print(f\"Status: {'PASS' if '20' in constrained_result4['text'] else 'FAIL'}\")\n",
        "\n",
        "# ========================================================================\n",
        "# SUMMARY\n",
        "# ========================================================================\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "tests = [\n",
        "    (\"Test 1: Basic arithmetic (x=5, 2x=?)\",\n",
        "     len(constrained_result['violations']) == 0),\n",
        "    (\"Test 2: Reduced violations vs baseline\",\n",
        "     len(constrained_result2['violations']) <= len(baseline_viols)),\n",
        "    (\"Test 3: Contradiction prevention (a=6, 2a=?)\",\n",
        "     len(constrained_result3['violations']) == 0),\n",
        "    (\"Test 4: Multi-variable (m=4,n=5,m*n=?)\",\n",
        "     len(constrained_result4['violations']) == 0)\n",
        "]\n",
        "\n",
        "passed = sum(1 for _, result in tests if result)\n",
        "\n",
        "for test_name, result in tests:\n",
        "    status = \"PASS\" if result else \"FAIL\"\n",
        "    print(f\"{status}: {test_name}\")\n",
        "\n",
        "print(f\"\\nOverall: {passed}/{len(tests)} tests passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoc4HISKPFWd"
      },
      "source": [
        "## Utility Functions and Analysis Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OERhxM_XPFWd",
        "outputId": "120a4913-c32a-48e4-e188-5562c60a35bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Evaluation utilities ready!\n",
            "  Sample dataset: 3 problems\n"
          ]
        }
      ],
      "source": [
        "def evaluate_on_dataset(decoder, problems: List[Dict], max_new_tokens=100):\n",
        "    \"\"\"\n",
        "    Evaluate decoder on a list of problems\n",
        "    Each problem should have 'prompt' and optionally 'expected_answer'\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for i, problem in enumerate(problems):\n",
        "        print(f\"\\nProblem {i+1}/{len(problems)}\")\n",
        "        print(f\"Prompt: {problem['prompt'][:100]}...\")\n",
        "\n",
        "        result = decoder.generate(\n",
        "            prompt=problem['prompt'],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            constraint_types=['arithmetic']\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            'problem': problem,\n",
        "            'output': result['text'],\n",
        "            'violations': result['violations'],\n",
        "            'stats': result['stats']\n",
        "        })\n",
        "\n",
        "        print(f\"Violations: {len(result['violations'])}\")\n",
        "        print(f\"Time: {result['stats']['time_seconds']:.2f}s\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def analyze_results(results: List[Dict]):\n",
        "    \"\"\"\n",
        "    Analyze evaluation results\n",
        "    \"\"\"\n",
        "    total = len(results)\n",
        "    violation_free = sum(1 for r in results if len(r['violations']) == 0)\n",
        "    avg_time = np.mean([r['stats']['time_seconds'] for r in results])\n",
        "    avg_pruned = np.mean([r['stats']['candidates_pruned'] for r in results])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EVALUATION SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Total problems: {total}\")\n",
        "    print(f\"Violation-free outputs: {violation_free} ({100*violation_free/total:.1f}%)\")\n",
        "    print(f\"Average generation time: {avg_time:.2f}s\")\n",
        "    print(f\"Average candidates pruned: {avg_pruned:.1f}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "# Example dataset for testing\n",
        "sample_problems = [\n",
        "    {\n",
        "        'prompt': \"Question: If x = 7, what is 3x?\\nAnswer:\",\n",
        "        'expected': 21\n",
        "    },\n",
        "    {\n",
        "        'prompt': \"Question: Sarah has 15 cookies. She eats 3. How many are left?\\nAnswer:\",\n",
        "        'expected': 12\n",
        "    },\n",
        "    {\n",
        "        'prompt': \"Question: A rectangle has length 5 and width 3. What is its area?\\nAnswer:\",\n",
        "        'expected': 15\n",
        "    },\n",
        "]\n",
        "\n",
        "print(\"✓ Evaluation utilities ready!\")\n",
        "print(f\"  Sample dataset: {len(sample_problems)} problems\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvXGn07ZPFWd"
      },
      "source": [
        "##  Evaluation on Sample Problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj6fpQr_PFWd"
      },
      "outputs": [],
      "source": [
        " results = evaluate_on_dataset(decoder, sample_problems, max_new_tokens=50)\n",
        " analyze_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBRQ7Zc0PFWd"
      },
      "source": [
        "## Optimization and Ablation Studies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhjAYmD4PFWd",
        "outputId": "948d7885-a729-482e-b2bc-997c7f619d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Ablation study functions ready!\n"
          ]
        }
      ],
      "source": [
        "def ablation_beam_width(prompt: str, beam_widths=[2, 4, 8]):\n",
        "    \"\"\"\n",
        "    Test impact of beam width on performance\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for width in beam_widths:\n",
        "        print(f\"\\nTesting beam width = {width}\")\n",
        "\n",
        "        decoder_temp = ConstraintGuidedDecoder(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            constraint_checker=ConstraintChecker(),\n",
        "            beam_width=width,\n",
        "            lambda_weight=1.0\n",
        "        )\n",
        "\n",
        "        result = decoder_temp.generate(prompt, max_new_tokens=50)\n",
        "        results[width] = result\n",
        "\n",
        "        print(f\"  Time: {result['stats']['time_seconds']:.2f}s\")\n",
        "        print(f\"  Violations: {len(result['violations'])}\")\n",
        "        print(f\"  Pruned: {result['stats']['candidates_pruned']}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def ablation_lambda_weight(prompt: str, lambda_values=[0.5, 1.0, 2.0]):\n",
        "    \"\"\"\n",
        "    Test impact of constraint strictness (lambda)\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for lambda_val in lambda_values:\n",
        "        print(f\"\\nTesting lambda = {lambda_val}\")\n",
        "\n",
        "        decoder_temp = ConstraintGuidedDecoder(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            constraint_checker=ConstraintChecker(),\n",
        "            beam_width=4,\n",
        "            lambda_weight=lambda_val\n",
        "        )\n",
        "\n",
        "        result = decoder_temp.generate(prompt, max_new_tokens=50)\n",
        "        results[lambda_val] = result\n",
        "\n",
        "        print(f\"  Score: {result['score']:.4f}\")\n",
        "        print(f\"  Violations: {len(result['violations'])}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"ablation study functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeETE4NJPFWd"
      },
      "source": [
        "## Save and Export Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-OizSGDPFWd",
        "outputId": "10dd95a6-82be-4e8c-f698-e2fbfd742f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Save functions ready!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def save_results(results: Dict, filename: str = None):\n",
        "    \"\"\"\n",
        "    Save experimental results to JSON\n",
        "    \"\"\"\n",
        "    if filename is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"results_{timestamp}.json\"\n",
        "\n",
        "    # convert sets to lists for JSON serialization\n",
        "    serializable_results = {}\n",
        "    for key, value in results.items():\n",
        "        if isinstance(value, dict):\n",
        "            serializable_results[key] = {\n",
        "                k: list(v) if isinstance(v, set) else v\n",
        "                for k, v in value.items()\n",
        "            }\n",
        "        else:\n",
        "            serializable_results[key] = value\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(serializable_results, f, indent=2)\n",
        "\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "print(\"Save functions ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWjOWxXLPFWe"
      },
      "source": [
        "## Interactive Testing Cell\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQQ4GA9IPFWe",
        "outputId": "50be7384-a7b5-4a8b-f2a3-102afd206071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "Question: If a train travels 60 miles in 2 hours, what is its speed?\n",
            "Answer: Let me calculate this. To find the speed, we need to divide the distance traveled by the time taken. In this case, the train traveled 60 miles in 2 hours. So, the speed would be 60 miles divided by 2 hours, which equals 30 miles per hour.\n",
            "\n",
            "Violations: []\n",
            "Generation time: 26.63s\n",
            "Candidates pruned: 0\n"
          ]
        }
      ],
      "source": [
        "# feel free to change this prompt and run to test\n",
        "custom_prompt = \"\"\"Question: If a train travels 60 miles in 2 hours, what is its speed?\n",
        "Answer: Let me calculate this.\"\"\"\n",
        "\n",
        "custom_result = decoder.generate(\n",
        "    prompt=custom_prompt,\n",
        "    max_new_tokens=50,\n",
        "    constraint_types=['arithmetic']\n",
        ")\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(custom_result['text'])\n",
        "print(f\"\\nViolations: {custom_result['violations']}\")\n",
        "print(f\"Generation time: {custom_result['stats']['time_seconds']:.2f}s\")\n",
        "print(f\"Candidates pruned: {custom_result['stats']['candidates_pruned']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12abba522a854f0d872136f5dd7a225c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51ecc6765e6e44c0b68d40c7781519f3",
              "IPY_MODEL_4bbffcbe357d44d6a26fc3af8623ade3",
              "IPY_MODEL_6b8e848ef389400eab53e5df05f3dca0"
            ],
            "layout": "IPY_MODEL_190612aefba541cdbbed86209be9a5b3"
          }
        },
        "51ecc6765e6e44c0b68d40c7781519f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72fe00d8ce4442bbb62055678e0bcd48",
            "placeholder": "​",
            "style": "IPY_MODEL_45084dd48697466f83f1db04859a5477",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4bbffcbe357d44d6a26fc3af8623ade3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab175e5bfa864d478a1f1a9c9e396234",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bb72a7c22814282815d43f51313f1ad",
            "value": 2
          }
        },
        "6b8e848ef389400eab53e5df05f3dca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1963f235c95e43e9b256ce253cf0fe0f",
            "placeholder": "​",
            "style": "IPY_MODEL_10202f216e6a472a8a481a653e973bc7",
            "value": " 2/2 [00:18&lt;00:00,  8.01s/it]"
          }
        },
        "190612aefba541cdbbed86209be9a5b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72fe00d8ce4442bbb62055678e0bcd48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45084dd48697466f83f1db04859a5477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab175e5bfa864d478a1f1a9c9e396234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bb72a7c22814282815d43f51313f1ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1963f235c95e43e9b256ce253cf0fe0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10202f216e6a472a8a481a653e973bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
